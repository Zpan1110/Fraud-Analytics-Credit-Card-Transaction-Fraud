# Executive Summary
One of the most common types of fraud is the credit card transaction fraud. This project aims to detect the fraudulent and synthetic information on credit card transactions in order to save the company’s potential asset loss due to this type of fraud. During the process, we conducted data cleaning to get rid of the empty columns and fill missing values on variables. We created variables to perform Benford’s Law and fussy match to identity the replicated values appearances. After that, we performed feature selection to achieve dimensionality reduction. By tuning the models with different parameters, we have obtained an out of time rate around 55% with FDR@3%. With the assumptions of saving $400 per each fraud detected and $20 loss for each false positive detection, the final model has reached to $21228000 for the total estimate saving, and we have decided to reject 3.5% of the transactions as our cutoff point to maximize the company’s saving. 
# Description of Data
The data is about Card Transactions. It contains the personal information of each recorded transactions. We aim to build an algorithm to detect the fraudulent credit card transactions from the data. The dataset covers the transaction records throughout the year 2010. It contains total of 10 fields and 96753 rows. 
# Variable Creation
The original dataset contains various transaction types, and we only focus on the transaction type = P in this project and filter out the empty columns. By creating the dictionary called “merchdes_merchnum”, we can map the merchandise description with merchandise numbers and use it to fill up the missing values from the variable “Merchnum”. With the similar method, we mapped State with matched zip code, merchandise description and merchandise number to fill the missing values of the variable “Merch State”. For those are not in the existing states, we assign them as “foreign”. We also used the same method for cleaning the variable “Merch zip” and assigned other missing values as “unknown”. 
Based on the Benford’s Law, we first created U_smoothed score on variables Merchnum and Cardnum to check the likelihood of synthetic records. We also used fuzzy matching on the combinations of original fields to check the replications of same value that appear across different fields and track how many times a certain value has appeared within the past {0,1,3,7,14,30} days. We used velocity change variables and velocity day since ratio to track the ratio change of a value versus its number of appearances within the past {1,3,7,14,30} days. 
# Feature Selection
With different combinations of original variables, we have expanded the dataset to a substantially high dimension. High dimensionality will cause trouble for fitting into a non-linear model, and the curse of dimensionality arises problems for analyzing the multicollinearity between variables when predicting the fraud label. Therefore, we use feature selection method to reduce the dimensionality by filtering out variables and eventually allow non-linear model optimize model architectures easier. We first set up the filter number (num_filter) and obtain the filter_score with KS test. Then we eliminate the variables that has lower correlations which are less important for predicting the response variable. After that, we use 25 wrappers on the remaining 300 features filtered from filter_score. Conducting forward selection method twice on each LGBM and Random Forest with different parameters by adding in each variable that would best fit in the model and we eventually see the performance curve flattened after adding the 8th variables with a score above 0.75. Then we ran a backward selection using Random Forest and the curve fluctuated between 0.71 and 0.72. The wrapper stacks the variable in each round, and we eventually find 25 variables to keep. 
# Preliminary Model Exploration
Among all the models provided, I eventually chose to tune 6 models with different parameters with variable number of 5,10,15, and compared their performance in terms of average percentage of fraud detected within 3% of population from 5 results each run. Based on the average results, logistic regression and single decision tree model detected the least percentage in both training, testing and out of time data. I noticed that the obtained results are more sensitive to parameter n_estimator in random forest model than LGBM. The overall results of each model are sensitive on the max_depth value (all around 5-7). In order to find if the model is over fitting of under fitting, we can compare the percentage of the fraud detected by training set and testing test to see if the difference is significant. In this case, my running results for models are above 75% which indicated that I need to be aware of the overfitting problems. The box plot summary shows the performance variance of each selected model which would help me identify the overfitting situations with further visualization. From the plot, we see that decision tree has largest variance and Catboost has lowest variance on out of time data. According to the outcomes, I found out the optimal variable size is 10, and I would like to use random forest for my final model and for further tuning. 

## The final model that I chose is random forest with parameters of 
model = RandomForestClassifier(n_estimators=20,max_depth=6,min_samples_split=35,min_samples_leaf=20,max_features=10,/ criterion ='entropy')
Based on the three result tables above, by applying the model within 3% of population transactions, the training set has obtained a fraud rate of 1.05% and detected 61.9% of fraud; the testing set obtained a fraud rate of 1.44% and detected 61.92% of fraud; the out of time set obtained a fraud rate of 1.05% and detected 63.49%.
